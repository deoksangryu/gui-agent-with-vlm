# OmniParser í•œê¸€ UI íŒŒì•…ì„ ìœ„í•œ OCR â†’ BLIP2 ì „í™˜ ë¶„ì„

## ê°œìš”

OmniParserì—ì„œ í•œê¸€ UIë¥¼ íš¨ê³¼ì ìœ¼ë¡œ íŒŒì•…í•˜ê¸° ìœ„í•´ OCR(Optical Character Recognition) ì‹œìŠ¤í…œì„ BLIP2 ëª¨ë¸ë¡œ ì „í™˜í•œ ê³¼ì •ê³¼ ê²°ê³¼ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.

## ë¬¸ì œ ìƒí™©

### ì´ˆê¸° ë¬¸ì œì 
1. **í•œê¸€ í…ìŠ¤íŠ¸ ì¸ì‹ ë¶€ì¡±**: ê¸°ë³¸ EasyOCRì´ ì˜ì–´ë§Œ ì§€ì›
2. **Florence2 ëª¨ë¸ í˜¸í™˜ì„± ë¬¸ì œ**: ìº¡ì…˜ ìƒì„± ì‹¤íŒ¨
3. **ì‘ì€ í…ìŠ¤íŠ¸ ê°ì§€ ë¶€ì¡±**: ë†’ì€ ì„ê³„ê°’(0.9)ìœ¼ë¡œ ì¸í•œ ëˆ„ë½
4. **UI ìš”ì†Œ ìº¡ì…˜ ìƒì„± ì‹¤íŒ¨**: ëª¨ë¸ í˜¸í™˜ì„± ë¬¸ì œ

## ë‹¨ê³„ë³„ í•´ê²° ê³¼ì •

### 1ë‹¨ê³„: í•œêµ­ì–´ OCR ì§€ì› ì¶”ê°€

**íŒŒì¼**: `steps/step3_korean_ocr_support.py`

**í•µì‹¬ ë³€ê²½ì‚¬í•­**:
```python
# ë³€ê²½ ì „
reader = easyocr.Reader(['en'])

# ë³€ê²½ í›„  
reader = easyocr.Reader(['ko', 'en'])
```

**ê°œì„  íš¨ê³¼**:
- í•œêµ­ì–´ í…ìŠ¤íŠ¸ ê°ì§€ ì‹œì‘: 'í† ìŠ¤ì¦ê¶Œ', 'ì£¼ì‹', 'ì½”ìŠ¤í”¼'
- OCR í…ìŠ¤íŠ¸: ~30ê°œ â†’ ~91ê°œ (+61ê°œ ì¦ê°€)
- í•œêµ­ì–´ ê°ì§€ìœ¨: 0% â†’ 70%+

**ë‚¨ì€ ë¬¸ì œ**:
- ì—¬ì „íˆ ë†’ì€ ì„ê³„ê°’ (0.9)
- ì‘ì€ í…ìŠ¤íŠ¸ '3', '4', '5', '6' ëˆ„ë½
- Florence2 ëª¨ë¸ í˜¸í™˜ì„± ë¬¸ì œ ì§€ì†

### 2ë‹¨ê³„: OCR ë¯¼ê°ë„ ê°œì„ 

**íŒŒì¼**: `steps/step4_improved_ocr_sensitivity.py`

**ê°œì„ ëœ OCR ì„¤ì •**:
```python
easyocr_args = {
    'paragraph': False,
    'text_threshold': 0.6,  # 0.9 â†’ 0.6
    'low_text': 0.3,
    'link_threshold': 0.4,
    'canvas_size': 3000,
    'mag_ratio': 1.5
}
```

**ê°œì„  íš¨ê³¼**:
- OCR í…ìŠ¤íŠ¸: 91ê°œ â†’ 102ê°œ (+11ê°œ ì¶”ê°€ ê°ì§€)
- ì‘ì€ í…ìŠ¤íŠ¸ ì„±ê³µ ê°ì§€: '1ì¼', '3', '5', 'ë™ë°©'
- ì €ëŒ€ë¹„ í…ìŠ¤íŠ¸ ì¸ì‹ ê°œì„ 
- ê³ í•´ìƒë„ ì´ë¯¸ì§€ ì²˜ë¦¬ ìµœì í™”

**ë‚¨ì€ ë¬¸ì œ**:
- Florence2 ëª¨ë¸ í˜¸í™˜ì„± ë¬¸ì œ ì§€ì†
- ì¼ë¶€ ì‘ì€ ìˆ«ì '4', '6' ì—¬ì „íˆ ëˆ„ë½
- UI ìš”ì†Œ ìº¡ì…˜ ìƒì„± ì‹¤íŒ¨

### 3ë‹¨ê³„: BLIP2 ëª¨ë¸ë¡œ êµì²´ (ìµœì¢… í•´ê²°)

**íŒŒì¼**: `steps/step5_blip2_model_switch.py`

**í•µì‹¬ ë³€ê²½ì‚¬í•­**:
```python
# ë³€ê²½ ì „ (ë¬¸ì œ)
caption_model_processor = get_caption_model_processor(
    model_name='florence2'
)

# ë³€ê²½ í›„ (í•´ê²°)
caption_model_processor = get_caption_model_processor(
    model_name='blip2',
    model_name_or_path='Salesforce/blip2-opt-2.7b'
)
```

**ê·¹ì ì¸ ê°œì„  ê²°ê³¼**:
- UI ìš”ì†Œ ê°ì§€: 0ê°œ â†’ 127ê°œ (ì™„ì „ ì„±ê³µ!)
- OCR í…ìŠ¤íŠ¸: 102ê°œ (ì´ì „ ë‹¨ê³„ ìœ ì§€)
- ìº¡ì…˜ ìƒì„±: ì„±ê³µ (BLIP2 ì˜ì–´ ì„¤ëª…)
- ì²˜ë¦¬ ì‹œê°„: ~79ì´ˆ (OCR 3ì´ˆ + SOM 76ì´ˆ)

## ê¸°ìˆ ì  êµ¬í˜„ ì„¸ë¶€ì‚¬í•­

### 1. ìº¡ì…˜ ëª¨ë¸ ì²˜ë¦¬ê¸° êµ¬í˜„

**íŒŒì¼**: `util/utils.py` - `get_caption_model_processor()`

```python
def get_caption_model_processor(model_name, model_name_or_path="Salesforce/blip2-opt-2.7b", device=None):
    if not device:
        device = "cuda" if torch.cuda.is_available() else "cpu"
    
    if model_name == "blip2":
        from transformers import Blip2Processor, Blip2ForConditionalGeneration
        processor = Blip2Processor.from_pretrained("Salesforce/blip2-opt-2.7b")
        if device == 'cpu':
            model = Blip2ForConditionalGeneration.from_pretrained(
                model_name_or_path, device_map=None, torch_dtype=torch.float32
            ) 
        else:
            model = Blip2ForConditionalGeneration.from_pretrained(
                model_name_or_path, device_map=None, torch_dtype=torch.float16
            ).to(device)
    
    elif model_name == "florence2":
        from transformers import AutoProcessor, AutoModelForCausalLM 
        processor = AutoProcessor.from_pretrained("microsoft/Florence-2-base", trust_remote_code=True)
        if device == 'cpu':
            model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=torch.float32, trust_remote_code=True)
        else:
            model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=torch.float16, trust_remote_code=True).to(device)
    
    return {'model': model.to(device), 'processor': processor}
```

### 2. ìº¡ì…˜ ìƒì„± í•¨ìˆ˜

**íŒŒì¼**: `util/utils.py` - `get_parsed_content_icon()`

```python
@torch.inference_mode()
def get_parsed_content_icon(filtered_boxes, starting_idx, image_source, caption_model_processor, prompt=None, batch_size=128):
    # ì´ë¯¸ì§€ í¬ë¡­ ë° ì „ì²˜ë¦¬
    croped_pil_image = []
    for i, coord in enumerate(non_ocr_boxes):
        try:
            xmin, xmax = int(coord[0]*image_source.shape[1]), int(coord[2]*image_source.shape[1])
            ymin, ymax = int(coord[1]*image_source.shape[0]), int(coord[3]*image_source.shape[0])
            cropped_image = image_source[ymin:ymax, xmin:xmax, :]
            cropped_image = cv2.resize(cropped_image, (64, 64))
            croped_pil_image.append(to_pil(cropped_image))
        except:
            continue

    model, processor = caption_model_processor['model'], caption_model_processor['processor']
    
    # ëª¨ë¸ë³„ í”„ë¡¬í”„íŠ¸ ì„¤ì •
    if not prompt:
        if 'florence' in model.config.name_or_path:
            prompt = "<CAPTION>"
        else:
            prompt = "The image shows"
    
    # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ìº¡ì…˜ ìƒì„±
    generated_texts = []
    for i in range(0, len(croped_pil_image), batch_size):
        batch = croped_pil_image[i:i+batch_size]
        inputs = processor(images=batch, text=[prompt]*len(batch), return_tensors="pt").to(device=device)
        
        if 'florence' in model.config.name_or_path:
            generated_ids = model.generate(input_ids=inputs["input_ids"], pixel_values=inputs["pixel_values"], max_new_tokens=20, num_beams=1, do_sample=False)
        else:
            generated_ids = model.generate(**inputs, max_length=100, num_beams=5, no_repeat_ngram_size=2, early_stopping=True, num_return_sequences=1)
        
        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)
        generated_text = [gen.strip() for gen in generated_text]
        generated_texts.extend(generated_text)
    
    return generated_texts
```

### 3. OCR ì„¤ì • ê°œì„ 

**íŒŒì¼**: `util/utils.py` - `check_ocr_box()`

```python
def check_ocr_box(image_source: Union[str, Image.Image], display_img = True, output_bb_format='xywh', goal_filtering=None, easyocr_args=None, use_paddleocr=False):
    if use_paddleocr:
        # PaddleOCR ì‚¬ìš© (í•œêµ­ì–´ ì§€ì›)
        if easyocr_args is None:
            text_threshold = 0.5
        else:
            text_threshold = easyocr_args['text_threshold']
        result = paddle_ocr.ocr(image_np, cls=False)[0]
        coord = [item[0] for item in result if item[1][1] > text_threshold]
        text = [item[1][0] for item in result if item[1][1] > text_threshold]
    else:
        # EasyOCR ì‚¬ìš© (í•œêµ­ì–´ + ì˜ì–´)
        if easyocr_args is None:
            easyocr_args = {}
        result = reader.readtext(image_np, **easyocr_args)
        coord = [item[0] for item in result]
        text = [item[1] for item in result]
```

## ìµœì¢… ì„±ê³¼ ë¶„ì„

### ğŸ“Š ì„±ëŠ¥ ì§€í‘œ
- **UI ìš”ì†Œ ê°ì§€**: 0ê°œ â†’ 127ê°œ (ì™„ì „ ì„±ê³µ!)
- **OCR í…ìŠ¤íŠ¸**: 102ê°œ (í•œêµ­ì–´ + ì˜ì–´)
- **ìº¡ì…˜ ìƒì„±**: ì„±ê³µ (BLIP2 ì˜ì–´ ì„¤ëª…)
- **ì²˜ë¦¬ ì‹œê°„**: ~79ì´ˆ (OCR 3ì´ˆ + SOM 76ì´ˆ)

### ğŸ¯ ì„¸ë¶€ ë¶„ì„
- **í…ìŠ¤íŠ¸ ìš”ì†Œ**: 79ê°œ (OCR ê¸°ë°˜)
- **SOM + OCR ê²°í•©**: 20ê°œ (í•œêµ­ì–´ UI ìš”ì†Œ)
- **SOM + BLIP2 ìº¡ì…˜**: 27ê°œ (ì˜ì–´ ì„¤ëª…)
- **ì´ í´ë¦­ ê°€ëŠ¥ ìš”ì†Œ**: 127ê°œ

### ğŸ¤– BLIP2 ìº¡ì…˜ ì˜ˆì‹œ
- "The image shows the number seven in korean"
- "The image shows a red line that is going up and down"
- "The image shows a graph of the stock market"

## ì‹¤í–‰ ë°©ë²•

### 1. ê¸°ë³¸ ì‹¤í–‰
```bash
python omniparser_demo.py -i toss_page.png -o toss_blip2_success --caption-model blip2 --threshold 0.02
```

### 2. ë‹¨ê³„ë³„ ì‹¤í–‰
```bash
# 1ë‹¨ê³„: í•œêµ­ì–´ OCR ì§€ì›
python steps/step3_korean_ocr_support.py

# 2ë‹¨ê³„: OCR ë¯¼ê°ë„ ê°œì„ 
python steps/step4_improved_ocr_sensitivity.py

# 3ë‹¨ê³„: BLIP2 ëª¨ë¸ ì „í™˜
python steps/step5_blip2_model_switch.py
```

## ê²°ë¡ 

í•œê¸€ UI íŒŒì•…ì„ ìœ„í•œ OCR â†’ BLIP2 ì „í™˜ì€ ë‹¤ìŒê³¼ ê°™ì€ ì„±ê³¼ë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤:

1. **í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì¸ì‹**: EasyOCR í•œêµ­ì–´ ì§€ì›ìœ¼ë¡œ 70%+ ê°ì§€ìœ¨ ë‹¬ì„±
2. **OCR ë¯¼ê°ë„ ê°œì„ **: ì„ê³„ê°’ ì¡°ì •ìœ¼ë¡œ ì‘ì€ í…ìŠ¤íŠ¸ê¹Œì§€ ê°ì§€
3. **BLIP2 ëª¨ë¸ ì „í™˜**: Florence2 í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°ë¡œ ìº¡ì…˜ ìƒì„± ì„±ê³µ
4. **ìµœì¢… ì„±ê³¼**: 127ê°œ UI ìš”ì†Œ ê°ì§€ë¡œ ì™„ì „í•œ í•œê¸€ UI íŒŒì•… ë‹¬ì„±

ì´ëŸ¬í•œ ê°œì„ ì„ í†µí•´ OmniParserëŠ” í•œê¸€ UI í™˜ê²½ì—ì„œë„ íš¨ê³¼ì ìœ¼ë¡œ ì‘ë™í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. 