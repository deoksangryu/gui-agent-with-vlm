{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoProcessor\n",
    "from vllm.multimodal.utils import fetch_image\n",
    "from PIL import Image, ImageDraw\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import ast\n",
    "\n",
    "def display(image, figsize=(10, 20)):\n",
    "    if isinstance(image, str):\n",
    "        image = Image.open(BytesIO(requests.get(image).content)) if image.startswith('http') else Image.open(image)\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "min_pixels = 256*28*28\n",
    "max_pixels = 1344*28*28\n",
    "temperature=0.9\n",
    "top_p=0.9\n",
    "max_tokens=512\n",
    "gpu_num=1 # 4 # allow multi-gpu inference\n",
    "\n",
    "_SYSTEM = \"Based on the screenshot of the page, I give a text description and you give its corresponding location. The coordinate represents a clickable location [x, y] for an element, which is a relative coordinate on the screenshot, scaled from 0 to 1.\"\n",
    "prompt=\"Delete Carnation Breakfast EssentialsNutritional Drink (Pack of 8) from the cart.\"\n",
    "image_url=\"examples/web_shopping.png\"\n",
    "display(image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapt from https://github.com/vllm-project/vllm/blob/main/examples/offline_inference/vision_language_multi_image.py\n",
    "def inference(question, image_url: str):\n",
    "    try:\n",
    "        from qwen_vl_utils import process_vision_info\n",
    "    except ModuleNotFoundError:\n",
    "        print('WARNING: `qwen-vl-utils` not installed, input images will not '\n",
    "              'be automatically resized. You can enable this functionality by '\n",
    "              '`pip install qwen-vl-utils`.')\n",
    "        process_vision_info = None\n",
    "\n",
    "    model_name = \"showlab/ShowUI-2B\"\n",
    "\n",
    "    llm = LLM(\n",
    "        model=model_name,\n",
    "        max_model_len=32768 if process_vision_info is None else 4096,\n",
    "        tensor_parallel_size=gpu_num,\n",
    "        max_num_seqs=5,\n",
    "        limit_mm_per_prompt={\"image\": 1},\n",
    "    )\n",
    "\n",
    "    placeholder = [{\"type\": \"image\", \"image\": image_url, \"min_pixels\": min_pixels, \"max_pixels\": max_pixels}]\n",
    "    messages = [{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": _SYSTEM\n",
    "    }, {\n",
    "        \"role\":\n",
    "        \"user\",\n",
    "        \"content\": [\n",
    "            *placeholder,\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": question\n",
    "            },\n",
    "        ],\n",
    "    }]\n",
    "\n",
    "    processor = AutoProcessor.from_pretrained(model_name)\n",
    "\n",
    "    prompt = processor.apply_chat_template(messages,\n",
    "                                           tokenize=False,\n",
    "                                           add_generation_prompt=True)\n",
    "\n",
    "    stop_token_ids = None\n",
    "\n",
    "    if process_vision_info is None:\n",
    "        image_data = fetch_image(image_url)\n",
    "    else:\n",
    "        image_data, _ = process_vision_info(messages)\n",
    "        \n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        max_tokens=max_tokens,\n",
    "        stop_token_ids=stop_token_ids\n",
    "    )\n",
    "\n",
    "    outputs = llm.generate(\n",
    "        {\n",
    "            \"prompt\": prompt,\n",
    "            \"multi_modal_data\": {\n",
    "                \"image\": image_data\n",
    "            },\n",
    "        },\n",
    "        sampling_params=sampling_params)\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "def draw_point(image_input, point=None, radius=5):\n",
    "    if isinstance(image_input, str):\n",
    "        image = Image.open(BytesIO(requests.get(image_input).content)) if image_input.startswith('http') else Image.open(image_input)\n",
    "    else:\n",
    "        image = image_input\n",
    "\n",
    "    if point:\n",
    "        x, y = point[0] * image.width, point[1] * image.height\n",
    "        ImageDraw.Draw(image).ellipse((x - radius, y - radius, x + radius, y + radius), fill='red')\n",
    "    display(image)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = inference(prompt, image_url)\n",
    "output_text = outputs[0].outputs[0].text\n",
    "print(output_text)\n",
    "click_xy = ast.literal_eval(output_text)\n",
    "draw_point(image_url, click_xy, 15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
