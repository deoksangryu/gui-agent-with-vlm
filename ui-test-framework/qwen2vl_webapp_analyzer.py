"""
Qwen2VL ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ ë¶„ì„ê¸°

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ ìŠ¤í¬ë¦°ìƒ· ë¶„ì„ì— íŠ¹í™”ëœ Qwen2VL ê¸°ë°˜ ë„êµ¬ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ë³´ì¶©ì„¤ëª…ì„ ë°›ì•„ ë” ì •í™•í•˜ê³  ë§¥ë½ì ì¸ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
1. ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ ìŠ¤í¬ë¦°ìƒ· ì „ë¬¸ ë¶„ì„
2. ì‚¬ìš©ì ë³´ì¶©ì„¤ëª… í†µí•© ë¶„ì„
3. ì›¹ UI/UX ì „ë¬¸ ê´€ì  ì œê³µ
4. ì‚¬ìš©ì ì›Œí¬í”Œë¡œìš° ë¶„ì„
5. ì›¹ ì ‘ê·¼ì„± ë° ì‚¬ìš©ì„± í‰ê°€
"""

import os
import torch
import time
from PIL import Image
from io import BytesIO
import requests
from qwen_vl_utils import process_vision_info
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor

# GPU ìµœì í™” ì„¤ì •
torch.backends.cuda.enable_mem_efficient_sdp(False)
torch.backends.cuda.enable_flash_sdp(False)

class WebAppAnalyzer:
    """
    ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ ìŠ¤í¬ë¦°ìƒ· ì „ë¬¸ ë¶„ì„ê¸°
    """
    
    def __init__(self, model_name="Qwen/Qwen2-VL-2B-Instruct", device="auto"):
        """
        ì›¹ì•± ë¶„ì„ê¸° ì´ˆê¸°í™”
        
        Args:
            model_name: ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„
            device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ (auto, cpu, cuda, mps ë“±)
        """
        print("ğŸŒ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ ë¶„ì„ê¸° ì´ˆê¸°í™” ì¤‘...")
        
        # ì´ë¯¸ì§€ ì²˜ë¦¬ ì„¤ì •
        self.min_pixels = 256 * 28 * 28
        self.max_pixels = 1344 * 28 * 28
        
        # ë””ë°”ì´ìŠ¤ ì„¤ì •
        if device == "auto":
            if torch.cuda.is_available():
                self.device = "cuda"
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                self.device = "mps"
            else:
                self.device = "cpu"
        else:
            self.device = device
        
        print(f"ğŸ“± ì‚¬ìš© ë””ë°”ì´ìŠ¤: {self.device}")
        
        try:
            # í”„ë¡œì„¸ì„œ ë° ëª¨ë¸ ë¡œë“œ
            self.processor = AutoProcessor.from_pretrained(
                model_name,
                min_pixels=self.min_pixels,
                max_pixels=self.max_pixels
            )
            
            if self.device == "cpu":
                self.model = Qwen2VLForConditionalGeneration.from_pretrained(
                    model_name,
                    torch_dtype=torch.float32,
                    device_map="cpu"
                )
            else:
                self.model = Qwen2VLForConditionalGeneration.from_pretrained(
                    model_name,
                    torch_dtype=torch.bfloat16,
                    device_map=self.device
                )
            
            print("âœ… ì›¹ì•± ë¶„ì„ê¸° ì¤€ë¹„ ì™„ë£Œ!")
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    
    def load_image(self, image_input):
        """ì´ë¯¸ì§€ ë¡œë“œ í•¨ìˆ˜"""
        if isinstance(image_input, str):
            if image_input.startswith('http'):
                response = requests.get(image_input)
                image = Image.open(BytesIO(response.content))
            else:
                image = Image.open(image_input)
        else:
            image = image_input
        return image
    
    def analyze_webapp_screenshot(self, image_input, user_context=""):        
        # ê¸°ë³¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
        system_prompt = """
ë‹¹ì‹ ì€ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ UI/UX ì „ë¬¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤. 
ì œê³µëœ ìŠ¤í¬ë¦°ìƒ·ì€ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ í™”ë©´ì…ë‹ˆë‹¤.
ì‚¬ìš©ìê°€ ì œê³µí•œ ë³´ì¶©ì„¤ëª…ê³¼ í•¨ê»˜ ì „ë¬¸ì ì´ê³  ì‹¤ìš©ì ì¸ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”.
"""
        task_prompt = """
í´ë¦­í•  ìš”ì†Œë“¤ì„ ,ë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´ë¡œ ì •ë¦¬
"""
        # ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ í†µí•©
        if user_context.strip():
            context_section = f"""

ã€ì‚¬ìš©ì ì œê³µ ì •ë³´ã€‘
{user_context.strip()}

ìœ„ ì •ë³´ë¥¼ ê³ ë ¤í•˜ì—¬ ë”ìš± ì •í™•í•˜ê³  ë§ì¶¤í˜• ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”.
"""
            full_prompt = system_prompt + task_prompt + context_section
        else:
            full_prompt = system_prompt + task_prompt
        
        return self._generate_response(image_input, full_prompt)
    
    def _generate_response(self, image_input, prompt):
        """Qwen2VL ì‘ë‹µ ìƒì„± (ë‚´ë¶€ ë©”ì„œë“œ)"""
        try:
            start_time = time.time()
            
            # ì´ë¯¸ì§€ ë¡œë“œ
            image = self.load_image(image_input)
            
            # ë©”ì‹œì§€ êµ¬ì„±
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {
                            "type": "image",
                            "image": image_input if isinstance(image_input, str) else image,
                            "min_pixels": self.min_pixels,
                            "max_pixels": self.max_pixels,
                        },
                    ],
                }
            ]
            
            # í…œí”Œë¦¿ ì ìš© ë° ì²˜ë¦¬
            text = self.processor.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            
            image_inputs, video_inputs = process_vision_info(messages)
            
            inputs = self.processor(
                text=[text],
                images=image_inputs,
                videos=video_inputs,
                padding=True,
                return_tensors="pt",
            )
            
            inputs = inputs.to(self.device)
            
            # ì‘ë‹µ ìƒì„±
            print("ğŸ”„ ì›¹ì•± ë¶„ì„ ì¤‘...")
            generated_ids = self.model.generate(
                **inputs, 
                max_new_tokens=4000,  # ë” ê¸´ ì‘ë‹µì„ ìœ„í•´ ì¦ê°€
                do_sample=True,
                temperature=0.7,
                top_p=0.9
            )
            
            # ê²°ê³¼ ë””ì½”ë”©
            generated_ids_trimmed = [
                out_ids[len(in_ids):] 
                for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
            ]
            
            output_text = self.processor.batch_decode(
                generated_ids_trimmed, 
                skip_special_tokens=True, 
                clean_up_tokenization_spaces=False
            )[0]
            
            end_time = time.time()
            print(f"â±ï¸ ë¶„ì„ ì™„ë£Œ ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
            
            return output_text.strip()
            
        except Exception as e:
            return f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

def quick_demo():
    """
    ë¹ ë¥¸ ë°ëª¨ ì‹¤í–‰
    """
    print("ğŸš€ ì›¹ì•± ë¶„ì„ê¸° ë¹ ë¥¸ ë°ëª¨")
    print("=" * 40)
    
    try:
        analyzer = WebAppAnalyzer()
    except Exception as e:
        print(f"âŒ ë¶„ì„ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return
    
    # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€
    image_path = "test_screenshot.png"
    
    if not os.path.exists(image_path):
        print(f"âš ï¸ í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_path}")
        print("ğŸ’¡ 'test_screenshot.png' íŒŒì¼ì„ ì¤€ë¹„í•´ì£¼ì„¸ìš”.")
        return
    
    # ì˜ˆì œ ë¶„ì„
    print(f"ğŸ“¸ ì´ë¯¸ì§€ ë¶„ì„: {image_path}")
    
    # ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ì˜ˆì œ
    user_context = "ì´ê²ƒì€ í† ìŠ¤ì¦ê¶Œ ë©”ì¸ í™”ë©´ì…ë‹ˆë‹¤."
    
    print(f"ğŸ’¬ ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸: {user_context}")
    print("\nğŸ”„ ë¹ ë¥¸ ë¶„ì„ ì‹¤í–‰ ì¤‘...")
    
    try:
        result = analyzer.analyze_webapp_screenshot(image_path, user_context)
        print(f"\nğŸ“ ë¶„ì„ ê²°ê³¼:")
        print("="*50)
        print(result)
        print("="*50)
        print("âœ… ë°ëª¨ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ ë°ëª¨ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    print("ğŸŒ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ ì „ë¬¸ ë¶„ì„ê¸°")
    quick_demo()